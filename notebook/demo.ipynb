{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37312938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set up imports from src/\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "from src.attention import MultiHeadAttention\n",
    "from src.encoder import EncoderLayer\n",
    "from src.decoder import DecoderLayer\n",
    "from src.positional_encodings import PositionalEncoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with dummy data\n",
    "B, T, D = 2, 10, 512 # Batch size, sequence length, embedding dimension\n",
    "dummy_input = torch.randn(B,T,D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d641557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional encoding added. Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "pos_enc = PositionalEncoding(d_model=D)\n",
    "output = pos_enc(dummy_input)\n",
    "\n",
    "print(\"Positional encoding added. Output shape:\", output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962501e8",
   "metadata": {},
   "source": [
    "We use sinusoidal encodings to inject order into our input sequence, since the Transformer has no built-in recurrence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "122de87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head attention output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "attn = MultiHeadAttention(d_model=D, num_heads=8)\n",
    "out = attn(dummy_input, dummy_input, dummy_input)\n",
    "\n",
    "print(\"Multi-head attention output shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373dc6b",
   "metadata": {},
   "source": [
    "Here, we project the input into Query, Key and Value vectors compute scaled dot-product attention, and concatenate the heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed3abf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder layer output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderLayer(d_model=D, num_heads=8, d_ff=2048)\n",
    "enc_output = encoder(dummy_input)\n",
    "\n",
    "print(\"Encoder layer output shape:\", enc_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c395ec",
   "metadata": {},
   "source": [
    "The encoder applies self-attention + feedforward network, wrapped in residual connections and layer normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f90c704e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder layer output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "decoder = DecoderLayer(d_model=D, num_heads=8, d_ff=2048)\n",
    "dec_output = decoder(dummy_input, enc_output)\n",
    "\n",
    "print(\"Decoder layer output shape:\", dec_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f80cf",
   "metadata": {},
   "source": [
    "The decoder combines:\n",
    " Masked self-attention (can't look ahead)\n",
    " Encoder-decoder attention\n",
    "-Feedforward network\n",
    "\n",
    "It enables sequence generation by learning dependencies on previously generated tokens and encoder output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbfe803",
   "metadata": {},
   "source": [
    "Testing the transformer with some data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37a31fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'fr']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/tiny_translation.csv\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93474fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file found!\n",
      "CSV columns: ['en', 'fr']\n",
      "First few rows:\n",
      "                         en                                  fr\n",
      "0                    hello                             bonjour\n",
      "1              how are you                       comment Ã§a va\n",
      "2  the cat sits on the mat      le chat est assis sur le tapis\n",
      "3  i love machine learning  j'aime l'apprentissage automatique\n",
      "4             good morning                             bonjour\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "from src.preprocessing import load_translation_data, build_vocab, tensorize\n",
    "\n",
    "csv_path = \"../data/tiny_translation.csv\"\n",
    "\n",
    "# Debug: Check CSV file\n",
    "if os.path.exists(csv_path):\n",
    "    print(\"CSV file found!\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV columns:\", df.columns.tolist())\n",
    "    print(\"First few rows:\\n\", df.head())\n",
    "else:\n",
    "    print(\"CSV file not found at:\", csv_path)\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "src_sents = df['en'].tolist()\n",
    "tgt_sents = df['fr'].tolist()\n",
    "src_vocab = build_vocab(src_sents)\n",
    "tgt_vocab = build_vocab(tgt_sents)\n",
    " \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
